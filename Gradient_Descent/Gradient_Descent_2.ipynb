{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Descent_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnho-nduEL9M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(x):\n",
        "  return x**2 + 10 * np.sin(x)\n",
        "\n",
        "def grad(x):\n",
        "  return 2*x + 10*np.cos(x)"
      ],
      "metadata": {
        "id": "AI71O4ITQS4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_coverged(theta_new, grad):\n",
        "  return np.linalg.norm(grad(theta_new)) / len(theta_new) < 1e-3\n",
        "\n",
        "def GD_momentum(theta_init, grad, eta, gamma):\n",
        "  theta = [theta_init]\n",
        "  v_old = np.zeros_like(theta_init)\n",
        "  for i in range(100):\n",
        "    v_new = gamma*v_old + eta*grad(theta[-1])\n",
        "    theta_new = theta[-1] - v_new\n",
        "    if has_coverged(theta_new, grad):\n",
        "      break\n",
        "    theta.append(theta_new)\n",
        "    v_old = v_new\n",
        "  return theta"
      ],
      "metadata": {
        "id": "YGq0tN4tQddW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function Of Linear Regression With Stochastic Gradient Descent."
      ],
      "metadata": {
        "id": "O6T4dgwBSZA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgrad(w, i, rd_id):\n",
        "    true_i = rd_id[i]\n",
        "    xi = Xbar[true_i, :]\n",
        "    yi = y[true_i]\n",
        "    a = np.dot(xi, w) - yi\n",
        "    return (xi*a).reshape(2, 1)\n",
        "\n",
        "def SGD(w_init, grad, eta):\n",
        "    w = [w_init]\n",
        "    w_last_check = w_init\n",
        "    iter_check_w = 10\n",
        "    N = X.shape[0]\n",
        "    count = 0\n",
        "    for it in range(10):\n",
        "        # shuffle data \n",
        "        rd_id = np.random.permutation(N)\n",
        "        for i in range(N):\n",
        "            count += 1 \n",
        "            g = sgrad(w[-1], i, rd_id)\n",
        "            w_new = w[-1] - eta*g\n",
        "            w.append(w_new)\n",
        "            if count%iter_check_w == 0:\n",
        "                w_this_check = w_new                 \n",
        "                if np.linalg.norm(w_this_check - w_last_check)/len(w_init) < 1e-3:                                    \n",
        "                    return w\n",
        "                w_last_check = w_this_check\n",
        "    return w"
      ],
      "metadata": {
        "id": "oziaqJtB1g-r"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}